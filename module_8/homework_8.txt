# https://courses.cs.washington.edu/courses/cse446/18wi/sections/section8/XOR-Pytorch.html

import torch
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt

from graphviz import Digraph
# make_dot was moved to https://github.com/szagoruyko/pytorchviz
from torchviz import make_dot

%matplotlib inline
torch.manual_seed(2)

X = torch.Tensor([[0,0],[0,1], [1,0], [1,1]])
Y = torch.Tensor([0,1,1,0]).view(-1,1)

X

Y

class XOR(nn.Module):
    def __init__(self, input_dim = 2, output_dim=1):
        super(XOR, self).__init__()
        self.lin1 = nn.Linear(input_dim, 2)
        self.lin2 = nn.Linear(2, output_dim)
    
    def forward(self, x):
        x = self.lin1(x)
        x = F.sigmoid(x)
        x = self.lin2(x)
        return x

model = XOR()

def weights_init(model):
    for m in model.modules():
        if isinstance(m, nn.Linear):
            # initialize the weight tensor, here we use a normal distribution
            m.weight.data.normal_(0, 1)

weights_init(model)

loss_func = nn.MSELoss()

optimizer = optim.SGD(model.parameters(), lr=0.02, momentum=0.9)

epochs = 2001
steps = X.size(0)
for i in range(epochs):
    for j in range(steps):
        data_point = np.random.randint(X.size(0))
        x_var = Variable(X[data_point], requires_grad=False)
        y_var = Variable(Y[data_point], requires_grad=False)
        
        optimizer.zero_grad()
        y_hat = model(x_var)
        loss = loss_func.forward(y_hat, y_var)
        loss.backward()
        optimizer.step()
        
    if (i % 500 == 0):
        print("Epoch: {0}, Loss: {1}, ".format(i, loss.data.numpy()))

model_params = list(model.parameters())

model_weights = model_params[0].data.numpy()
model_bias = model_params[1].data.numpy()

plt.scatter(X.numpy()[[0,-1], 0], X.numpy()[[0, -1], 1], s=50)
plt.scatter(X.numpy()[[1,2], 0], X.numpy()[[1, 2], 1], c='red', s=50)

x_1 = np.arange(-0.1, 1.1, 0.1)
y_1 = ((x_1 * model_weights[0,0]) + model_bias[0]) / (-model_weights[0,1])
plt.plot(x_1, y_1)

x_2 = np.arange(-0.1, 1.1, 0.1)
y_2 = ((x_2 * model_weights[1,0]) + model_bias[1]) / (-model_weights[1,1])
plt.plot(x_2, y_2)
plt.legend(["neuron_1", "neuron_2"], loc=8)
plt.show()

# https://stackoverflow.com/questions/54916135/what-is-the-class-definition-of-nn-linear-in-pytorch

# y = x.matmul(m.weight.t()) + m.bias  # y = x*W^T + b

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

X = torch.Tensor([[1, 1, 1],
                  [1, 1, 0],
                  [1, 0, 1],
                  [1, 0, 0],
                  [0, 1, 1],
                  [0, 1, 0],
                  [0, 0, 1],
                  [0, 0, 0]])

Y = torch.Tensor([0,0,0,0,0,1,0,0]).view(-1,1)

class Problem1(nn.Module):
    def __init__(self, input_dim = 3):
        super(Problem1, self).__init__()
        self.lin1 = nn.Linear(input_dim, 1)
    
    def forward(self, x):
        x = self.lin1(x)
        x = F.sigmoid(x)
        return x

model1 = Problem1()

def weights_init(model):
    for m in model.modules():
        if isinstance(m, nn.Linear):
            # initialize the weight tensor, here we use a normal distribution
            m.weight.data.normal_(0, 1)

weights_init(model1)

loss_func = nn.MSELoss()

optimizer = optim.SGD(model1.parameters(), lr=0.02, momentum=0.9)

epochs = int(1e4 + 1.)
steps = X.size(0)
for i in range(epochs):
    for j in range(steps):
        data_point = np.random.randint(X.size(0))
        x_var = Variable(X[data_point], requires_grad=False)
        y_var = Variable(Y[data_point], requires_grad=False)
        
        optimizer.zero_grad()
        y_hat = model1(x_var)
        loss = loss_func.forward(y_hat, y_var)
        loss.backward()
        optimizer.step()
        
    if (i % 1e3 == 0):
        print("Epoch: {0}, Loss: {1}, ".format(i, loss.data.numpy()))

for i in range(len(Y)):
    print(model1(X[i,]))

Y

model_params = list(model1.parameters())

model_weights = model_params[0].data.numpy()
model_bias = model_params[1].data.numpy()

model_weights

model_bias

sigmoid(X[0,] @ model_weights.T + model_bias)

# https://www.programmersought.com/article/479977857/
model_params = list(model1.parameters())

vis_graph = make_dot(model1(X[1,]), params=dict(model1.named_parameters()))
vis_graph.view()

# x1, x2, x3...!x2...!x2x3
# 1, 1, 1; 0; 0
# 1, 1, 0; 0; 0
# 1, 0, 1; 1; 1
# 1, 0, 0; 1; 0
# 0, 1, 1; 0; 0
# 0, 1, 0; 0; 0
# 0, 0, 1; 1; 1
# 0, 0, 0; 1; 0

X = torch.Tensor([[1, 1, 1],
                  [1, 1, 0],
                  [1, 0, 1],
                  [1, 0, 0],
                  [0, 1, 1],
                  [0, 1, 0],
                  [0, 0, 1],
                  [0, 0, 0]])

Y = torch.Tensor([0,0,1,0,0,0,1,0]).view(-1,1)

class Problem2(nn.Module):
    def __init__(self, input_dim = 3):
        super(Problem2, self).__init__()
        self.lin1 = nn.Linear(input_dim, 1)
    
    def forward(self, x):
        x = self.lin1(x)
        x = F.sigmoid(x)
        return x

model2 = Problem2()

def weights_init(model):
    for m in model.modules():
        if isinstance(m, nn.Linear):
            # initialize the weight tensor, here we use a normal distribution
            m.weight.data.normal_(0, 1)

weights_init(model2)

loss_func = nn.MSELoss()

optimizer = optim.SGD(model2.parameters(), lr=0.02, momentum=0.9)

epochs = int(1e4 + 1.)
steps = X.size(0)
for i in range(epochs):
    for j in range(steps):
        data_point = np.random.randint(X.size(0))
        x_var = Variable(X[data_point], requires_grad=False)
        y_var = Variable(Y[data_point], requires_grad=False)
        
        optimizer.zero_grad()
        y_hat = model2(x_var)
        loss = loss_func.forward(y_hat, y_var)
        loss.backward()
        optimizer.step()
        
    if (i % 1e3 == 0):
        print("Epoch: {0}, Loss: {1}, ".format(i, loss.data.numpy()))

for i in range(len(Y)):
    print(model2(X[i,]))

Y

model_params = list(model2.parameters())

model_weights = model_params[0].data.numpy()
model_bias = model_params[1].data.numpy()

model_weights

model_bias

sigmoid(X[0,] @ model_weights.T + model_bias)

# x1, x2, x3...ans
# 1, 1, 1; 1
# 1, 1, 0; 0
# 1, 0, 1; 0
# 1, 0, 0; 0
# 0, 1, 1; 0
# 0, 1, 0; 0
# 0, 0, 1; 0
# 0, 0, 0; 1

X = torch.Tensor([[1, 1, 1],
                  [1, 1, 0],
                  [1, 0, 1],
                  [1, 0, 0],
                  [0, 1, 1],
                  [0, 1, 0],
                  [0, 0, 1],
                  [0, 0, 0]])

Y = torch.Tensor([1,0,0,0,0,0,0,1]).view(-1,1)

class Problem3(nn.Module):
    def __init__(self, input_dim = 3, output_dim=1, hidden_dim=5):
        super(Problem3, self).__init__()
        self.lin1 = nn.Linear(input_dim, hidden_dim)
        self.lin2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = self.lin1(x)
        x = F.sigmoid(x)
        x = self.lin2(x)
        x = F.sigmoid(x)
        return x

model3 = Problem3()

def weights_init(model):
    for m in model.modules():
        if isinstance(m, nn.Linear):
            # initialize the weight tensor, here we use a normal distribution
            m.weight.data.normal_(0, 1)

weights_init(model3)

loss_func = nn.MSELoss()

optimizer = optim.SGD(model3.parameters(), lr=0.02, momentum=0.9)

epochs = int(1e4 + 1.)
steps = X.size(0)
for i in range(epochs):
    for j in range(steps):
        data_point = np.random.randint(X.size(0))
        x_var = Variable(X[data_point], requires_grad=False)
        y_var = Variable(Y[data_point], requires_grad=False)
        
        optimizer.zero_grad()
        y_hat = model3(x_var)
        loss = loss_func.forward(y_hat, y_var)
        loss.backward()
        optimizer.step()
        
    if (i % 5e3 == 0):
        print("Epoch: {0}, Loss: {1}, ".format(i, loss.data.numpy()))

for i in range(len(Y)):
    print(model3(X[i,]))
#     print(model3(X[i,]), F.sigmoid(F.sigmoid(X[i,] @ W1.T + b1) @ W2.T + b2))

Y

model_params = list(model3.parameters())

model_params

# W1; (10x3) b1; (10,1); W2; (1,10); b2; (1,1)

model_params[0].shape, model_params[1].shape, model_params[2].shape, model_params[3].shape

W1 = model_params[0].data.numpy()
b1 = model_params[1].data.numpy()
W2 = model_params[2].data.numpy()
b2 = model_params[3].data.numpy()

F.sigmoid(F.sigmoid(X[0,] @ W1.T + b1) @ W2.T + b2)

# x1, x2, x3...ans
# 1, 1, 1; 1
# 1, 1, 0; 1
# 1, 0, 1; 1
# 1, 0, 0; 1
# 0, 1, 1; 0
# 0, 1, 0; 0
# 0, 0, 1; 0
# 0, 0, 0; 0

X = torch.Tensor([[1, 1, 1],
                  [1, 1, 0],
                  [1, 0, 1],
                  [1, 0, 0],
                  [0, 1, 1],
                  [0, 1, 0],
                  [0, 0, 1],
                  [0, 0, 0]])

Y = torch.Tensor([1,1,1,1,0,0,0,0]).view(-1,1)

class Problem4(nn.Module):
    def __init__(self, input_dim = 3, output_dim=1, hidden_dim=5):
        super(Problem4, self).__init__()
        self.lin1 = nn.Linear(input_dim, hidden_dim)
        self.lin2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = self.lin1(x)
        x = F.sigmoid(x)
        x = self.lin2(x)
        x = F.sigmoid(x)
        return x

model4 = Problem4()

def weights_init(model):
    for m in model.modules():
        if isinstance(m, nn.Linear):
            # initialize the weight tensor, here we use a normal distribution
            m.weight.data.normal_(0, 1)

weights_init(model4)

loss_func = nn.MSELoss()

optimizer = optim.SGD(model4.parameters(), lr=0.02, momentum=0.9)

epochs = int(1e4 + 1.)
steps = X.size(0)
for i in range(epochs):
    for j in range(steps):
        data_point = np.random.randint(X.size(0))
        x_var = Variable(X[data_point], requires_grad=False)
        y_var = Variable(Y[data_point], requires_grad=False)
        
        optimizer.zero_grad()
        y_hat = model4(x_var)
        loss = loss_func.forward(y_hat, y_var)
        loss.backward()
        optimizer.step()
        
    if (i % 5e3 == 0):
        print("Epoch: {0}, Loss: {1}, ".format(i, loss.data.numpy()))

for i in range(len(Y)):
    print(model4(X[i,]))

Y

model_params = list(model4.parameters())

model_params

# W1; (10x3) b1; (10,1); W2; (1,10); b2; (1,1)

model_params[0].shape, model_params[1].shape, model_params[2].shape, model_params[3].shape

W1 = model_params[0].data.numpy()
b1 = model_params[1].data.numpy()
W2 = model_params[2].data.numpy()
b2 = model_params[3].data.numpy()

F.sigmoid(F.sigmoid(X[0,] @ W1.T + b1) @ W2.T + b2)

# x1, x2, x3...ans
# 1, 1, 1; 0
# 1, 1, 0; 0
# 1, 0, 1; 0
# 1, 0, 0; 1
# 0, 1, 1; 0
# 0, 1, 0; 1
# 0, 0, 1; 0
# 0, 0, 0; 0

X = torch.Tensor([[1, 1, 1],
                  [1, 1, 0],
                  [1, 0, 1],
                  [1, 0, 0],
                  [0, 1, 1],
                  [0, 1, 0],
                  [0, 0, 1],
                  [0, 0, 0]])

Y = torch.Tensor([0,0,0,1,0,1,0,0]).view(-1,1)

class Problem5(nn.Module):
    def __init__(self, input_dim = 3, output_dim=1, hidden_dim=5):
        super(Problem5, self).__init__()
        self.lin1 = nn.Linear(input_dim, hidden_dim)
        self.lin2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = self.lin1(x)
        x = F.sigmoid(x)
        x = self.lin2(x)
        x = F.sigmoid(x)
        return x

model5 = Problem5()

def weights_init(model):
    for m in model.modules():
        if isinstance(m, nn.Linear):
            # initialize the weight tensor, here we use a normal distribution
            m.weight.data.normal_(0, 1)

weights_init(model5)

loss_func = nn.MSELoss()

optimizer = optim.SGD(model5.parameters(), lr=0.02, momentum=0.9)

epochs = int(1e4 + 1.)
steps = X.size(0)
for i in range(epochs):
    for j in range(steps):
        data_point = np.random.randint(X.size(0))
        x_var = Variable(X[data_point], requires_grad=False)
        y_var = Variable(Y[data_point], requires_grad=False)
        
        optimizer.zero_grad()
        y_hat = model5(x_var)
        loss = loss_func.forward(y_hat, y_var)
        loss.backward()
        optimizer.step()
        
    if (i % 5e3 == 0):
        print("Epoch: {0}, Loss: {1}, ".format(i, loss.data.numpy()))

for i in range(len(Y)):
    print(model5(X[i,]))

Y

model_params = list(model5.parameters())

model_params

model_params[0].shape, model_params[1].shape, model_params[2].shape, model_params[3].shape

W1 = model_params[0].data.numpy()
b1 = model_params[1].data.numpy()
W2 = model_params[2].data.numpy()
b2 = model_params[3].data.numpy()

F.sigmoid(F.sigmoid(X[0,] @ W1.T + b1) @ W2.T + b2)

# https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py

# https://www.programmersought.com/article/479977857/
model_params = list(model5.parameters())

vis_graph = make_dot(model5(X[1,]), params=dict(model5.named_parameters()))

vis_graph.view()

import pandas as pd

pd.set_option('display.max_rows', 500)

np.random.seed(6)

sample11 = np.random.normal(loc=1, scale=np.sqrt(0.2), size=(100,1))
sample12 = np.random.normal(loc=-1, scale=np.sqrt(0.2), size=(100,1))

sample21 = np.random.normal(loc=0, scale=np.sqrt(0.2), size=(100,1))
sample22 = np.random.normal(loc=0, scale=np.sqrt(0.2), size=(100,1))

sample1 = pd.DataFrame({
    'x11':[i[0] for i in sample11],
    'x12':[i[0] for i in sample12]
    })

sample2 = pd.DataFrame({
    'x21':[i[0] for i in sample21],
    'x22':[i[0] for i in sample22]
    })

sum(sample1.x11 - sample1.x12 < 1)

sum(sample2.x21 - sample2.x22 > 1)

bad_indices1 = sample1[sample1.x11 - sample1.x12 < 1].index
sample1.iloc[bad_indices1,1] = sample1.iloc[bad_indices1,1] - 0.3
sum(sample1.x11 - sample1.x12 < 1)

bad_indices2 = sample2[sample2.x21 - sample2.x22 > 1].index
sample2.iloc[bad_indices2, 1] = sample2.iloc[bad_indices2, 1] + 0.5
sum(sample2.x21 - sample2.x22 > 1)

class_list = [0] * 100
class_list.extend([1] * 100)

sample1.columns = ['x1', 'x2']
sample2.columns = ['x1', 'x2']
sample_data = pd.concat([sample1, sample2], axis=0, ignore_index=True)

sample_data['Class'] = class_list

sample_data.head()

X = torch.Tensor(np.array(sample_data.loc[:,['x1','x2']]))
Y = torch.Tensor(np.array(sample_data.Class)).view(-1,1)

class Problem6(nn.Module):
    def __init__(self, input_dim = 2):
        super(Problem6, self).__init__()
        self.lin1 = nn.Linear(input_dim, 1)
    
    def forward(self, x):
        x = self.lin1(x)
        x = F.sigmoid(x)
        return x

model6 = Problem6()

def weights_init(model):
    for m in model.modules():
        if isinstance(m, nn.Linear):
            # initialize the weight tensor, here we use a normal distribution
            m.weight.data.normal_(0, 1)

weights_init(model6)

loss_func = nn.MSELoss()

optimizer = optim.SGD(model6.parameters(), lr=0.02, momentum=0.9)

epochs = int(1e4 + 1.)
steps = X.size(0)
for i in range(epochs):
    for j in range(steps):
        data_point = np.random.randint(X.size(0))
        x_var = Variable(X[data_point], requires_grad=False)
        y_var = Variable(Y[data_point], requires_grad=False)
        
        optimizer.zero_grad()
        y_hat = model6(x_var)
        loss = loss_func.forward(y_hat, y_var)
        loss.backward()
        optimizer.step()
        
    if (i % 1e3 == 0):
        print("Epoch: {0}, Loss: {1}, ".format(i, loss.data.numpy()))

for i in range(len(Y)):
    print(model6(X[i,]))

model6(X).data.numpy() > 0.5

Y

model_params = list(model6.parameters())

model_weights = model_params[0].data.numpy()
model_bias = model_params[1].data.numpy()

w1, w2, b

w1 = model_weights[0][0]
w2 = model_weights[0][1]
b = model_bias[0]

# Reference: https://medium.com/@thomascountz/calculate-the-decision-boundary-of-a-single-perceptron-visualizing-linear-separability-c4d77099ef38
slope = -(b / w2) / (b / w1)
y_int = -b / w2
x = np.linspace(-1,3)

from matplotlib.pyplot import figure
figure(num=None, figsize=(10, 10), dpi=80, facecolor='w', edgecolor='k')

plt.figure(figsize=(10,10))
plt.suptitle('Problem 6', fontsize=20)
plt.xlabel('x1', fontsize=18)
plt.ylabel('x2', fontsize=16)
plt.scatter(sample_data.iloc[0:100,0], sample_data.iloc[0:100,1])
plt.scatter(sample_data.iloc[100:,0], sample_data.iloc[100:,1], c='red')
plt.plot(x, slope * x + y_int, c='black')
plt.savefig('p6.jpg')

plt.scatter(X.numpy()[[0,-1], 0], X.numpy()[[0, -1], 1], s=50)
plt.scatter(X.numpy()[[1,2], 0], X.numpy()[[1, 2], 1], c='red', s=50)

x_1 = np.arange(-0.1, 1.1, 0.1)
y_1 = ((x_1 * model_weights[0,0]) + model_bias[0]) / (-model_weights[0,1])
plt.plot(x_1, y_1)

x_2 = np.arange(-0.1, 1.1, 0.1)
y_2 = ((x_2 * model_weights[1,0]) + model_bias[1]) / (-model_weights[1,1])
plt.plot(x_2, y_2)
plt.legend(["neuron_1", "neuron_2"], loc=8)
plt.show()
